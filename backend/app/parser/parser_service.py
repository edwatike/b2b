import logging
import asyncio
from typing import List, Dict, Any, Set, Optional
from app.models.search_result import SearchResult
from app.db import async_session
from sqlalchemy.exc import SQLAlchemyError
from tenacity import retry, stop_after_attempt, wait_exponential
from datetime import datetime
from sqlmodel import select
from bs4 import BeautifulSoup
import traceback
import sys
import json

from .models import SearchRequest, SearchResponse
from .playwright_runner import PlaywrightRunner
from .search_google import GoogleSearch
from .config.parser_config import config
from .utils import (
    extract_domain,
    extract_emails,
    extract_phones,
    clean_text,
    clean_url
)
from ..database import async_session

logger = logging.getLogger(__name__)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.DEBUG)

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=2, min=1, max=10),
    reraise=True
)
async def search_and_save(keyword: str, max_results: int = 1000) -> List[Dict[str, Any]]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ ScraperAPI –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        keyword: –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞
        max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000)
        
    Returns:
        List[Dict[str, Any]]: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        
    Raises:
        Exception: –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    logger.info(f"üîç –ù–ê–ß–ò–ù–ê–ï–ú –ü–û–ò–°–ö –ü–û –ó–ê–ü–†–û–°–£: {keyword}")
    print(f"üîç –ù–ê–ß–ò–ù–ê–ï–ú –ü–û–ò–°–ö –ü–û –ó–ê–ü–†–û–°–£: {keyword}")
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Playwright
        playwright_runner = PlaywrightRunner()
        try:
            logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Playwright...")
            await playwright_runner.initialize()
            logger.info("‚úÖ Playwright —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ SearchEngine —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º Playwright
            search_engine = SearchEngine(playwright_runner)
            logger.info("–ó–∞–ø—É—Å–∫ –ø–æ–∏—Å–∫–∞ –≤ Google...")
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL-–∞–¥—Ä–µ—Å–æ–≤
            try:
                # –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–¥ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –ø—Ä—è–º–æ–π –∑–∞–ø—É—Å–∫ Playwright
                logger.info("üß™ –¢–ï–°–¢–û–í–´–ô –ó–ê–ü–£–°–ö: –ø—Ä—è–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Playwright –¥–ª—è –ø–æ–∏—Å–∫–∞")
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–∑ –ø—É–ª–∞
                page = await playwright_runner.get_page()
                logger.info("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª—É—á–µ–Ω–∞ –∏–∑ –ø—É–ª–∞")
                
                try:
                    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –æ—Ç–∫—Ä—ã–≤–∞–µ–º Google –Ω–∞–ø—Ä—è–º—É—é
                    logger.info("–ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ Google...")
                    content = await playwright_runner.navigate(page, "https://www.google.ru")
                    
                    if content:
                        logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ Google")
                        logger.debug(f"–ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {content[:500]}")
                    else:
                        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É Google")
                        
                    # –í–≤–æ–¥–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                    logger.info(f"–í–≤–æ–¥–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {keyword}")
                    await playwright_runner.type_search_query(page, keyword, "textarea[name='q']")
                    logger.info("‚úÖ –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –≤–≤–µ–¥–µ–Ω")
                    
                    # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    logger.info("–û–∂–∏–¥–∞–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
                    await page.wait_for_load_state("networkidle", timeout=60000)
                    logger.info("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                    content = await page.content()
                    if content:
                        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Ä–∞–∑–º–µ—Ä: {len(content)} –±–∞–π—Ç)")
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                        with open("/tmp/google_results.html", "w", encoding="utf-8") as f:
                            f.write(content)
                        logger.info("‚úÖ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ /tmp/google_results.html")
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        soup = BeautifulSoup(content, 'html.parser')
                        logger.info("–ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ...")
                        
                        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        test_selectors = [
                            'div.yuRUbf > a',
                            '.kCrYT > a',
                            'a.l',
                            'div.g div.r a',
                            'div.tF2Cxc a',
                            'div.g div.yuRUbf a',
                            'a.cz3goc',
                            '.DhN8Cf a',
                            '.LC20lb',
                            'h3'  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        ]
                        
                        all_results = []
                        for selector in test_selectors:
                            results = soup.select(selector)
                            logger.info(f"–°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(results)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                            
                            if results:
                                for i, result in enumerate(results[:5]):  # –í—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                                    href = result.get('href') if hasattr(result, 'get') else None
                                    text = result.text if hasattr(result, 'text') else str(result)
                                    logger.info(f"  üìå –†–µ–∑—É–ª—å—Ç–∞—Ç #{i+1}: href={href}, text={text[:100]}")
                                    
                                    if href and href.startswith('http'):
                                        all_results.append({
                                            "url": href,
                                            "title": text.strip() if text else "",
                                            "snippet": "",
                                            "position": i+1
                                        })
                        
                        logger.info(f"‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_results)}")
                        
                        # –î–µ–ª–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                        screenshot_path = "/tmp/google_results.png"
                        await page.screenshot(path=screenshot_path, full_page=True)
                        logger.info(f"‚úÖ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {screenshot_path}")
                        
                        return all_results[:max_results]
                    else:
                        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                        return []
                
                except Exception as e:
                    error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å Google: {str(e)}"
                    logger.error(error_msg)
                    logger.error(f"Traceback: {traceback.format_exc()}")
                    return []
                
                finally:
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—Ä–∞—Ç–Ω–æ –≤ –ø—É–ª
                    logger.info("–í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –ø—É–ª...")
                    await playwright_runner.release_page(page)
                    logger.info("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∞ –≤ –ø—É–ª")
                    
            except Exception as e:
                error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {str(e)}"
                logger.error(error_msg)
                logger.error(f"Traceback: {traceback.format_exc()}")
                return []
        
        finally:
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã Playwright
            logger.info("–û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã Playwright...")
            await playwright_runner.cleanup()
            logger.info("‚úÖ –†–µ—Å—É—Ä—Å—ã Playwright –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã")
    
    except Exception as e:
        error_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        logger.error(error_msg)
        logger.error(f"Traceback: {traceback.format_exc()}")
        return []

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=2, min=1, max=10),
    reraise=True
)
async def scrape_url(url: str) -> str:
    """
    –ü–∞—Ä—Å–∏—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π URL —á–µ—Ä–µ–∑ ScraperAPI —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º retry
    
    Args:
        url: URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        
    Returns:
        str: HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ URL: {url}")
    html = await scrape_url_with_scraperapi(url)
    if not html:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è {url}")
        return ""
    logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω HTML –¥–ª—è {url}")
    return html

class ParserService:
    def __init__(self):
        self.playwright_runner = None
        self.google_search = None
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã"""
        self.playwright_runner = PlaywrightRunner()
        await self.playwright_runner.initialize()
        self.google_search = GoogleSearch(self.playwright_runner)
        logger.info("‚úÖ Parser service –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
    async def cleanup(self):
        """–û—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç –≤—Å–µ —Ä–µ—Å—É—Ä—Å—ã"""
        if self.playwright_runner:
            await self.playwright_runner.cleanup()
        logger.info("‚úÖ Parser service –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω")
    
    async def search(self, request: SearchRequest) -> SearchResponse:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
        
        Args:
            request: –û–±—ä–µ–∫—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–∏—Å–∫–∞
            
        Returns:
            –û–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {request.query}")
        
        total_results = 0
        cached_count = 0
        new_count = 0
        all_results = []
        
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = SearchResponse(results=[], total=0, cached=0, new=0)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google
            logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫ –≤ Google –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{request.query}'")
            search_results = await self.google_search.search(
                query=request.query,
                region=request.region,
                limit=request.limit
            )
            
            if search_results:
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Google")
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç SearchResult –¥–ª—è –≤–µ–±-API
                web_results = []
                for result in search_results:
                    web_result = SearchResult(
                        url=result.url,
                        title=result.title,
                        snippet=result.snippet,
                        position=result.position,
                        source="google"
                    )
                    web_results.append(web_result)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
                response.results = web_results
                response.total = len(web_results)
                response.new = len(web_results)
                
                logger.info(f"‚úÖ –ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ: –Ω–∞–π–¥–µ–Ω–æ {response.total} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            else:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Google")
                
            return response
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {str(e)}")
            logger.error(traceback.format_exc())
            return SearchResponse(results=[], total=0, cached=0, new=0)
        
    async def __aenter__(self):
        await self.initialize()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.cleanup()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(test_scraperapi())
